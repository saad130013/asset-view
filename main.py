
import streamlit as st
import pandas as pd
import torch
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel

st.set_page_config(page_title="Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª BERT Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ğŸ¤– Ù†Ø¸Ø§Ù… Ø§Ù‚ØªØ±Ø§Ø­ ÙˆØµÙ Ø§Ù„Ø£ØµÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df = pd.read_excel("assetv4.xlsx", header=1)
df.columns = df.columns.str.strip()
asset_descriptions = df["Asset Description For Maintenance Purpose"].dropna().astype(str).unique().tolist()

# ØªØ­Ù…ÙŠÙ„ BERT Ø§Ù„Ø¹Ø±Ø¨ÙŠ
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("asafaya/bert-base-arabic")
    model = AutoModel.from_pretrained("asafaya/bert-base-arabic")
    return tokenizer, model

tokenizer, model = load_model()

# Ø¯Ø§Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ù† BERT
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=32)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding

# ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù…Ø³Ø¨Ù‚Ø§Ù‹
@st.cache_resource
def embed_all_descriptions():
    embeddings = []
    for desc in asset_descriptions:
        try:
            emb = get_embedding(desc)
            embeddings.append(emb)
        except:
            embeddings.append([0]*768)
    return embeddings

description_embeddings = embed_all_descriptions()

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
user_input = st.text_input("âœï¸ Ø§ÙƒØªØ¨ ÙˆØµÙ Ø§Ù„Ø£ØµÙ„")

if user_input:
    query_vec = get_embedding(user_input)
    similarities = cosine_similarity([query_vec], description_embeddings)[0]
    top_indices = similarities.argsort()[-3:][::-1]
    st.markdown("### ğŸ’¡ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø°ÙƒÙŠØ© Ù…Ù† BERT:")
    for i in top_indices:
        st.markdown(f"- {asset_descriptions[i]} (ØªØ´Ø§Ø¨Ù‡: {similarities[i]:.2f})")
